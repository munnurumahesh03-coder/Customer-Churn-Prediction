{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/munnurumahesh03-coder/Customer-Churn-Prediction/blob/main/Churn_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaMPdN3i60K2"
      },
      "source": [
        "# **Automated Data Quality Reporting**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the ydata-profiling library quietly\n",
        "!pip install ydata-profiling -q"
      ],
      "metadata": {
        "id": "eQI-34XoyCQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9_OkWS3JnN9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from ydata_profiling import ProfileReport\n",
        "\n",
        "# The file is in our current directory, so we can just use its name\n",
        "file_path = 'WA_Fn-UseC_-Telco-Customer-Churn.csv'\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Generate and display the report\n",
        "profile = ProfileReport(df, title=\"Telco Customer Churn - Data Quality Report\")\n",
        "profile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4S4N_PW9_Gz"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T221LJJO7sd7"
      },
      "source": [
        "# **Initial Data Assessment (Manual)**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evVnnFnLJnLn"
      },
      "outputs": [],
      "source": [
        "# Display a concise summary of the DataFrame.\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9k7vhY_7A56"
      },
      "outputs": [],
      "source": [
        "# Generate descriptive statistics for numerical columns.\n",
        "print(\"\\n--- Descriptive Statistics (Numerical Columns) ---\")\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyYkPBKz7A2Z"
      },
      "outputs": [],
      "source": [
        "# Check the distribution of the 'Churn' column.\n",
        "print(\"\\n--- Target Variable Distribution (Churn) ---\")\n",
        "df['Churn'].value_counts(normalize=True) * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb_3qOhxGHbs"
      },
      "source": [
        "# **Exploratory Data Analysis (EDA) & Outlier Strategy**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuX5uC4V-HCp"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set a professional and aesthetically pleasing style for the plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6) # Set default figure size\n",
        "\n",
        "print(\"âœ… Libraries imported and plot style set.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6t5mBF5j-G_9"
      },
      "outputs": [],
      "source": [
        "# Create a count plot to show churn distribution across different contract types\n",
        "sns.countplot(data=df, x='Contract', hue='Churn')\n",
        "\n",
        "# Add a title and labels for clarity\n",
        "plt.title('Churn Rate by Contract Type', fontsize=16)\n",
        "plt.xlabel('Contract Type', fontsize=12)\n",
        "plt.ylabel('Number of Customers', fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBmC-gpz-Gz8"
      },
      "outputs": [],
      "source": [
        "# Create a count plot for churn distribution by Internet Service type\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=df, x='InternetService', hue='Churn')\n",
        "\n",
        "# Add titles and labels for clarity\n",
        "plt.title('Churn Distribution by Internet Service', fontsize=16)\n",
        "plt.xlabel('Internet Service Type', fontsize=12)\n",
        "plt.ylabel('Number of Customers', fontsize=12)\n",
        "plt.legend(title='Churn')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtAJQ0CD-GvI"
      },
      "outputs": [],
      "source": [
        "# Create a histogram of 'tenure' differentiated by 'Churn'\n",
        "# 'multiple=\"stack\"' will stack the 'Yes' and 'No' bars on top of each other.\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.histplot(data=df, x='tenure', hue='Churn', multiple='stack', bins=30, palette={'Yes': '#ff7f0e', 'No': '#1f77b4'})\n",
        "\n",
        "# Add titles and labels for clarity\n",
        "plt.title('Churn Distribution by Customer Tenure', fontsize=16)\n",
        "plt.xlabel('Tenure (in Months)', fontsize=12)\n",
        "plt.ylabel('Number of Customers', fontsize=12)\n",
        "plt.legend(title='Churn', labels=['Yes', 'No']) # Manually set labels to match colors\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrIMUT6V-Gsg"
      },
      "outputs": [],
      "source": [
        "# Create a histogram of 'MonthlyCharges' differentiated by 'Churn'\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.histplot(data=df, x='MonthlyCharges', hue='Churn', multiple='stack', bins=30, palette={'Yes': '#ff7f0e', 'No': '#1f77b4'})\n",
        "\n",
        "# Add titles and labels for clarity\n",
        "plt.title('Churn Distribution by Monthly Charges', fontsize=16)\n",
        "plt.xlabel('Monthly Charges ($)', fontsize=12)\n",
        "plt.ylabel('Number of Customers', fontsize=12)\n",
        "plt.legend(title='Churn', labels=['Yes', 'No']) # Manually set labels to match colors\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UxWTIZ5-Gih"
      },
      "outputs": [],
      "source": [
        "# Create a figure with two subplots, arranged side-by-side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
        "\n",
        "# --- Box Plot for 'tenure' ---\n",
        "sns.boxplot(data=df, y='tenure', ax=axes[0])\n",
        "axes[0].set_title('Box Plot of Customer Tenure', fontsize=14)\n",
        "axes[0].set_ylabel('Tenure (in Months)', fontsize=12)\n",
        "\n",
        "# --- Box Plot for 'MonthlyCharges' ---\n",
        "sns.boxplot(data=df, y='MonthlyCharges', ax=axes[1])\n",
        "axes[1].set_title('Box Plot of Monthly Charges', fontsize=14)\n",
        "axes[1].set_ylabel('Monthly Charges ($)', fontsize=12)\n",
        "\n",
        "# Adjust the layout to prevent titles/labels from overlapping\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Cleaning & Feature Engineering**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "W-7tTqsgsW5M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxCG5Z_G-Gc7"
      },
      "outputs": [],
      "source": [
        "# --- Fix TotalCharges ---\n",
        "\n",
        "# First, let's verify the problem again.\n",
        "print(\"--- Before Cleaning ---\")\n",
        "print(f\"Data type of TotalCharges: {df['TotalCharges'].dtype}\")\n",
        "problematic_rows_count = (df['TotalCharges'] == ' ').sum()\n",
        "print(f\"Number of rows with spaces in TotalCharges: {problematic_rows_count}\")\n",
        "\n",
        "# The 'errors=\"coerce\"' argument will automatically turn any non-numeric values (like our spaces) into NaN.\n",
        "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
        "\n",
        "# The logical fix: If tenure is 0, the total charges should also be 0.\n",
        "# We will fill the resulting NaN values with 0.\n",
        "df['TotalCharges'].fillna(0, inplace=True)\n",
        "\n",
        "# --- Verification ---\n",
        "print(\"\\n--- After Cleaning ---\")\n",
        "print(f\"Data type of TotalCharges now: {df['TotalCharges'].dtype}\")\n",
        "print(f\"Number of missing values in TotalCharges now: {df['TotalCharges'].isnull().sum()}\")\n",
        "print(\"âœ… Cleaning of 'TotalCharges' is complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvT9F1iX-GaA"
      },
      "outputs": [],
      "source": [
        "# --- Drop customerID Column ---\n",
        "\n",
        "print(f\"--- Before Dropping ---\")\n",
        "print(f\"Number of columns in the DataFrame: {df.shape[1]}\")\n",
        "print(f\"Does 'customerID' exist in columns? {'customerID' in df.columns}\")\n",
        "\n",
        "\n",
        "# Drop the 'customerID' column.\n",
        "# 'axis=1' specifies that we are dropping a column, not a row.\n",
        "# 'inplace=True' modifies the DataFrame directly.\n",
        "df.drop('customerID', axis=1, inplace=True)\n",
        "\n",
        "\n",
        "# --- Verification ---\n",
        "print(f\"\\n--- After Dropping ---\")\n",
        "print(f\"Number of columns in the DataFrame now: {df.shape[1]}\")\n",
        "print(f\"Does 'customerID' exist in columns? {'customerID' in df.columns}\")\n",
        "print(\"âœ… 'customerID' column dropped successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iH8Dp6G7Aum"
      },
      "outputs": [],
      "source": [
        "# --- Feature Engineering ---\n",
        "\n",
        "print(\"--- Before Feature Engineering ---\")\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "\n",
        "# 1. Create 'TenureInYears'\n",
        "# We'll divide the tenure in months by 12 and round to two decimal places for neatness.\n",
        "df['TenureInYears'] = (df['tenure'] / 12).round(2)\n",
        "\n",
        "\n",
        "# --- Verification ---\n",
        "print(\"\\n--- After Feature Engineering ---\")\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "print(\"\\nPreview of the new 'TenureInYears' column:\")\n",
        "print(df[['tenure', 'TenureInYears']].head())\n",
        "print(\"\\nâœ… 'TenureInYears' feature created successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Create 'HasAdditionalServices' feature ---\n",
        "\n",
        "# First, define the list of columns that represent additional services.\n",
        "additional_service_cols = [\n",
        "    'OnlineSecurity',\n",
        "    'OnlineBackup',\n",
        "    'DeviceProtection',\n",
        "    'TechSupport',\n",
        "    'StreamingTV',\n",
        "    'StreamingMovies'\n",
        "]\n",
        "\n",
        "# Create the new feature.\n",
        "# We check if any of the service columns have the value 'Yes'.\n",
        "# The .any(axis=1) checks for any True values across each row.\n",
        "# .astype(int) converts the resulting True/False boolean into 1/0 integers.\n",
        "df['HasAdditionalServices'] = (df[additional_service_cols] == 'Yes').any(axis=1).astype(int)\n",
        "\n",
        "\n",
        "# --- Verification ---\n",
        "print(\"--- After Creating 'HasAdditionalServices' ---\")\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "print(\"\\nPreview of the new feature:\")\n",
        "# We'll show the new column alongside a few of the service columns to see how it works.\n",
        "print(df[['OnlineSecurity', 'StreamingTV', 'HasAdditionalServices']].head())\n",
        "\n",
        "# Let's see the distribution of this new feature.\n",
        "print(\"\\nDistribution of 'HasAdditionalServices':\")\n",
        "print(df['HasAdditionalServices'].value_counts(normalize=True))\n",
        "\n",
        "print(\"\\nâœ… 'HasAdditionalServices' feature created successfully.\")"
      ],
      "metadata": {
        "id": "OZ5Zlz4luqam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Create 'IsNewCustomer' feature ---\n",
        "\n",
        "# We'll define \"new\" as a tenure of 3 months or less.\n",
        "# This threshold is a reasonable choice based on our EDA histogram.\n",
        "df['IsNewCustomer'] = (df['tenure'] <= 3).astype(int)\n",
        "\n",
        "# --- Verification ---\n",
        "print(\"--- After Creating 'IsNewCustomer' ---\")\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "print(\"\\nPreview of the new feature:\")\n",
        "print(df[['tenure', 'IsNewCustomer']].head(10))\n",
        "\n",
        "print(\"\\nDistribution of 'IsNewCustomer':\")\n",
        "print(df['IsNewCustomer'].value_counts(normalize=True))\n",
        "\n",
        "print(\"\\nâœ… 'IsNewCustomer' feature created successfully.\")\n"
      ],
      "metadata": {
        "id": "2P7yjoTN5LMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Create 'HasNoInternetService' feature ---\n",
        "\n",
        "# This creates a binary flag that is 1 if InternetService is 'No', and 0 otherwise.\n",
        "df['HasNoInternetService'] = (df['InternetService'] == 'No').astype(int)\n",
        "\n",
        "# --- Verification ---\n",
        "print(\"--- After Creating 'HasNoInternetService' ---\")\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "print(\"\\nPreview of the new feature:\")\n",
        "print(df[['InternetService', 'HasNoInternetService']].head(10))\n",
        "\n",
        "print(\"\\nDistribution of 'HasNoInternetService':\")\n",
        "print(df['HasNoInternetService'].value_counts(normalize=True))\n",
        "\n",
        "print(\"\\nâœ… 'HasNoInternetService' feature created successfully.\")\n"
      ],
      "metadata": {
        "id": "byOfD_1i5O07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Create 'IsHighRiskContract' feature ---\n",
        "\n",
        "# This creates a binary flag that is 1 if Contract is 'Month-to-month', and 0 otherwise.\n",
        "df['IsHighRiskContract'] = (df['Contract'] == 'Month-to-month').astype(int)\n",
        "\n",
        "# --- Verification ---\n",
        "print(\"--- After Creating 'IsHighRiskContract' ---\")\n",
        "print(\"\\nPreview of the new feature:\")\n",
        "print(df[['Contract', 'IsHighRiskContract']].head(10))\n",
        "\n",
        "print(\"\\nDistribution of 'IsHighRiskContract':\")\n",
        "print(df['IsHighRiskContract'].value_counts(normalize=True))\n",
        "\n",
        "print(\"\\nâœ… 'IsHighRiskContract' feature created successfully.\")"
      ],
      "metadata": {
        "id": "jyY1fdnx5OxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Create 'TenureToMonthlyRatio' feature ---\n",
        "\n",
        "# To avoid division by zero, although we know MonthlyCharges are all positive,\n",
        "# it's good practice to add a small epsilon (a very small number).\n",
        "epsilon = 1e-6\n",
        "df['TenureToMonthlyRatio'] = df['tenure'] / (df['MonthlyCharges'] + epsilon)\n",
        "\n",
        "# --- Verification ---\n",
        "print(\"--- After Creating 'TenureToMonthlyRatio' ---\")\n",
        "print(\"\\nPreview of the new feature:\")\n",
        "print(df[['tenure', 'MonthlyCharges', 'TenureToMonthlyRatio']].head())\n",
        "\n",
        "print(\"\\nDescriptive statistics for the new feature:\")\n",
        "print(df['TenureToMonthlyRatio'].describe())\n",
        "\n",
        "print(\"\\nâœ… 'TenureToMonthlyRatio' feature created successfully.\")"
      ],
      "metadata": {
        "id": "Xm6xKXI-5Ou-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Encode the Target Variable 'Churn' ---\n",
        "\n",
        "print(\"--- Before Encoding ---\")\n",
        "print(\"Data type of 'Churn':\", df['Churn'].dtype)\n",
        "print(\"Unique values in 'Churn':\", df['Churn'].unique())\n",
        "\n",
        "# Encode 'Churn'\n",
        "# We will map 'Yes' to 1 and 'No' to 0.\n",
        "df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "# --- Verification ---\n",
        "print(\"\\n--- After Encoding ---\")\n",
        "print(\"Data type of 'Churn':\", df['Churn'].dtype)\n",
        "print(\"Unique values in 'Churn':\", df['Churn'].unique())\n",
        "print(\"\\nâœ… Target variable 'Churn' encoded successfully.\")"
      ],
      "metadata": {
        "id": "DWzZrVMpuqXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Splitting**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Srw2kXoMH52p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- Separate Features (X) and Target (y) ---\n",
        "\n",
        "# X contains all columns EXCEPT 'Churn'.\n",
        "X = df.drop('Churn', axis=1)\n",
        "# y contains ONLY the 'Churn' column.\n",
        "y = df['Churn']\n",
        "\n",
        "print(\"--- Initial Shapes ---\")\n",
        "print(\"Features (X) shape:\", X.shape)\n",
        "print(\"Target (y) shape:\", y.shape)"
      ],
      "metadata": {
        "id": "iTIiFm80uqKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- First Split (into Training + Validation and Test) ---\n",
        "\n",
        "# We'll first split the data into 80% for training/validation and 20% for the final test set.\n",
        "# We use 'stratify=y' to ensure that the proportion of churners (0s and 1s) is the same in both splits.\n",
        "# This is critical for imbalanced datasets like ours.\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(\"\\n--- After First Split ---\")\n",
        "print(\"Training/Validation set shape (X_train_val):\", X_train_val.shape)\n",
        "print(\"Test set shape (X_test):\", X_test.shape)"
      ],
      "metadata": {
        "id": "mWEhZ33XH8T3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Second Split (into Training and Validation) ---\n",
        "\n",
        "# Now we split the 80% 'X_train_val' set into our final training and validation sets.\n",
        "# The validation set should be 25% of this 80% chunk to equal 20% of the original total. (0.25 * 0.8 = 0.2)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val,\n",
        "    test_size=0.25,\n",
        "    random_state=42,\n",
        "    stratify=y_train_val\n",
        ")\n",
        "\n",
        "print(\"\\n--- After Second Split (Final Sets) ---\")\n",
        "print(\"Training set shape (X_train):\", X_train.shape)\n",
        "print(\"Validation set shape (X_val):\", X_val.shape)\n",
        "print(\"Test set shape (X_test):\", X_test.shape)"
      ],
      "metadata": {
        "id": "xH-J_zXcH8QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Verification ---\n",
        "print(\"\\n--- Final Distribution Verification ---\")\n",
        "print(f\"Original churn proportion:  {y.mean():.3f}\")\n",
        "print(f\"Training set churn proportion: {y_train.mean():.3f}\")\n",
        "print(f\"Validation set churn proportion: {y_val.mean():.3f}\")\n",
        "print(f\"Test set churn proportion:     {y_test.mean():.3f}\")\n",
        "print(\"\\nâœ… Data splitting is complete. The churn proportion is consistent across all sets.\")"
      ],
      "metadata": {
        "id": "K48zpKGCH8Nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Identify Feature Types ---\n",
        "\n",
        "# It's best practice to derive the lists from the training set (X_train).\n",
        "\n",
        "# 1. Identify all features that are already binary (0s and 1s)\n",
        "# These are our engineered features plus SeniorCitizen.\n",
        "binary_features = [col for col in X_train.columns if X_train[col].nunique() == 2 and X_train[col].min() == 0 and X_train[col].max() == 1]\n",
        "# We know SeniorCitizen is binary, so let's ensure it's in the list.\n",
        "if 'SeniorCitizen' not in binary_features:\n",
        "    binary_features.append('SeniorCitizen')\n",
        "\n",
        "# 2. Identify categorical features (text-based)\n",
        "# These are the columns with 'object' dtype.\n",
        "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "\n",
        "# 3. Identify continuous numerical features\n",
        "# These are all the columns that are NOT binary and NOT categorical.\n",
        "continuous_features = [\n",
        "    col for col in X_train.columns\n",
        "    if col not in binary_features and col not in categorical_features\n",
        "]\n",
        "\n",
        "# --- Verification ---\n",
        "print(\"--- Upgraded Feature Type Identification ---\")\n",
        "print(f\"Total number of features: {len(X_train.columns)}\")\n",
        "\n",
        "print(f\"\\nNumber of continuous features: {len(continuous_features)}\")\n",
        "print(\"Continuous features:\", continuous_features)\n",
        "\n",
        "print(f\"\\nNumber of binary features: {len(binary_features)}\")\n",
        "print(\"Binary features:\", binary_features)\n",
        "\n",
        "print(f\"\\nNumber of categorical features: {len(categorical_features)}\")\n",
        "print(\"Categorical features:\", categorical_features)\n",
        "\n",
        "print(\"\\nâœ… Advanced feature types identified and lists created successfully.\")"
      ],
      "metadata": {
        "id": "nI10RdcaH8LF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Advanced Pipeline Construction**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "PQ0o5DMDNYWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost -q"
      ],
      "metadata": {
        "id": "0BsyYElmuqHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# --- Define Preprocessing Steps ---\n",
        "\n",
        "# Pipeline for continuous features\n",
        "continuous_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Pipeline for categorical features (for CatBoost)\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
        "])\n",
        "\n",
        "\n",
        "# --- Create the ColumnTransformer ---\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('continuous', continuous_transformer, continuous_features),\n",
        "        ('categorical', categorical_transformer, categorical_features),\n",
        "        ('binary', 'passthrough', binary_features)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "\n",
        "# --- Create the Full CatBoost Pipeline ---\n",
        "\n",
        "# We chain the preprocessor, feature selector, and the GPU-enabled CatBoost model.\n",
        "model_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('feature_selection', SelectKBest(f_classif, k='all')),\n",
        "    (\n",
        "        'classifier',\n",
        "        CatBoostClassifier(\n",
        "            random_state=42,\n",
        "            verbose=0,\n",
        "            task_type='GPU'\n",
        "        )\n",
        "    )\n",
        "])\n",
        "\n",
        "# --- Verification ---\n",
        "print(\"--- Advanced Pipeline Construction (GPU Enabled) ---\")\n",
        "print(\"âœ… CatBoost installed.\")\n",
        "print(\"âœ… Preprocessing ColumnTransformer created successfully with imputers.\")\n",
        "print(\"âœ… Full model pipeline created with Feature Selection and GPU-enabled CatBoostClassifier.\")\n",
        "print(\"\\nPipeline structure:\")\n",
        "display(model_pipeline)"
      ],
      "metadata": {
        "id": "ZzI7SzrLL4db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Advanced Hyperparameter Tuning & Experiment Tracking**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "fkkaOFUiT2gO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install mlflow\n",
        "!pip install mlflow -q"
      ],
      "metadata": {
        "id": "CJraBylnyolk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Set Up MLflow ---\n",
        "\n",
        "import mlflow\n",
        "import os\n",
        "\n",
        "# Set the experiment name\n",
        "experiment_name = \"churn-prediction-v1\"\n",
        "mlflow.set_experiment(experiment_name)\n",
        "\n",
        "# Get the tracking URI (where the data will be stored)\n",
        "# This will create a directory called 'mlruns' in our project folder.\n",
        "tracking_uri = mlflow.get_tracking_uri()\n",
        "\n",
        "print(f\"âœ… MLflow experiment set to: '{experiment_name}'\")\n",
        "print(f\"âœ… MLflow tracking URI: '{tracking_uri}'\")\n",
        "print(f\"\\nYour experiment data will be stored in the '{os.path.basename(tracking_uri)}' directory.\")\n",
        "print(\"You can view the MLflow UI by running 'mlflow ui' in your terminal from the project directory.\")\n"
      ],
      "metadata": {
        "id": "x1qehpMmUI02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Optuna\n",
        "!pip install optuna -q"
      ],
      "metadata": {
        "id": "WsL3mLvP2rPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# --- Step 8.2: Define the Objective Function for Optuna ---\n",
        "\n",
        "def objective(trial):\n",
        "    \"\"\"\n",
        "    This function is called by Optuna for each trial.\n",
        "    It builds, trains, and evaluates a model with a given set of hyperparameters.\n",
        "    \"\"\"\n",
        "    # Use a 'with' statement to ensure the MLflow run is always closed\n",
        "    with mlflow.start_run(nested=True): # nested=True is good practice for Optuna\n",
        "\n",
        "        # --- 1. Define the Hyperparameter Search Space ---\n",
        "        # We use the 'trial' object to suggest values for each hyperparameter.\n",
        "        # The names ('feature_selection__k', 'classifier__depth') match the\n",
        "        # pipeline step names.\n",
        "\n",
        "        # a) Feature Selection\n",
        "        k = trial.suggest_int('feature_selection__k', 10, 25) # Number of features to select\n",
        "\n",
        "        # b) CatBoost Classifier Hyperparameters\n",
        "        depth = trial.suggest_int('classifier__depth', 4, 10)\n",
        "        learning_rate = trial.suggest_float('classifier__learning_rate', 0.01, 0.3, log=True)\n",
        "        l2_leaf_reg = trial.suggest_float('classifier__l2_leaf_reg', 1.0, 10.0, log=True)\n",
        "        iterations = trial.suggest_int('classifier__iterations', 100, 1000)\n",
        "\n",
        "        # --- 2. Log Parameters with MLflow ---\n",
        "        mlflow.log_params(trial.params)\n",
        "\n",
        "        # --- 3. Create the Pipeline with the Suggested Hyperparameters ---\n",
        "        # We re-define the pipeline inside the objective function to ensure\n",
        "        # each trial gets a fresh model with the new hyperparameters.\n",
        "        pipeline = Pipeline(steps=[\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('feature_selection', SelectKBest(f_classif, k=k)),\n",
        "            ('classifier', CatBoostClassifier(\n",
        "                depth=depth,\n",
        "                learning_rate=learning_rate,\n",
        "                l2_leaf_reg=l2_leaf_reg,\n",
        "                iterations=iterations,\n",
        "                random_state=42,\n",
        "                verbose=0,\n",
        "                task_type='GPU'\n",
        "            ))\n",
        "        ])\n",
        "\n",
        "        # --- 4. Train and Evaluate the Model ---\n",
        "        pipeline.fit(X_train, y_train)\n",
        "        y_pred_proba = pipeline.predict_proba(X_val)[:, 1] # Get probabilities for the positive class\n",
        "        auc_score = roc_auc_score(y_val, y_pred_proba)\n",
        "\n",
        "        # --- 5. Log Metrics with MLflow ---\n",
        "        mlflow.log_metric(\"validation_auc\", auc_score)\n",
        "\n",
        "        # --- 6. Return the Score to Optuna ---\n",
        "        # Optuna will try to maximize this value.\n",
        "        return auc_score\n",
        "\n",
        "print(\"âœ… Optuna objective function defined successfully.\")\n",
        "print(\"This function will now be used to run each hyperparameter tuning trial.\")"
      ],
      "metadata": {
        "id": "4MG_D3JxL4UG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Run the Optuna Optimization Study ---\n",
        "\n",
        "# We will wrap the entire study in a single parent MLflow run for better organization.\n",
        "with mlflow.start_run(run_name=\"Optuna-CatBoost-Tuning\"):\n",
        "    # Record the experiment ID and run ID for future reference\n",
        "    parent_run_id = mlflow.active_run().info.run_id\n",
        "    print(f\"Parent MLflow Run ID: {parent_run_id}\")\n",
        "    mlflow.log_param(\"tuning_method\", \"Optuna\")\n",
        "    mlflow.log_param(\"model_type\", \"CatBoostClassifier\")\n",
        "\n",
        "    # 1. Create the Optuna study\n",
        "    # We want to maximize the AUC, so the direction is 'maximize'.\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "\n",
        "    # 2. Start the optimization\n",
        "    # Optuna will call our 'objective' function 100 times.\n",
        "    # The progress bar will show you the status.\n",
        "    print(\"\\nStarting Optuna hyperparameter search for 100 trials...\")\n",
        "    study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
        "    print(\"\\nOptimization finished.\")\n",
        "\n",
        "    # --- Post-Tuning Analysis ---\n",
        "\n",
        "    # 3. Log the best trial's information to the parent run\n",
        "    best_params = study.best_trial.params\n",
        "    best_score = study.best_trial.value\n",
        "    mlflow.log_params(best_params)\n",
        "    mlflow.log_metric(\"best_validation_auc\", best_score)\n",
        "\n",
        "    # 4. Print the best results\n",
        "    print(\"\\n--- Best Trial Results ---\")\n",
        "    print(f\"ðŸ† Best AUC Score: {best_score:.4f}\")\n",
        "    print(\"ðŸ“‹ Best Hyperparameters:\")\n",
        "    for key, value in best_params.items():\n",
        "        print(f\"  - {key}: {value}\")\n",
        "\n",
        "print(\"\\nâœ… Step 8 complete. All trials have been logged to MLflow.\")"
      ],
      "metadata": {
        "id": "o9Q1CDydL4SN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final Model Training, Evaluation & Registration**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "rw64RjqjAi_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve Best Hyperparameters\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print(\"--- Step 9 (v2.0): Final Model Training & Registration with Signature ---\")\n",
        "print(\"ðŸ“‹ Using the best hyperparameters found by Optuna:\")\n",
        "for key, value in best_params.items():\n",
        "    print(f\"  - {key}: {value}\")\n",
        "\n",
        "X_train_full = pd.concat([X_train, X_val], axis=0)\n",
        "y_train_full = pd.concat([y_train, y_val], axis=0)\n",
        "\n",
        "final_model_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('feature_selection', SelectKBest(f_classif, k=best_params['feature_selection__k'])),\n",
        "    ('classifier', CatBoostClassifier(\n",
        "        depth=best_params['classifier__depth'],\n",
        "        learning_rate=best_params['classifier__learning_rate'],\n",
        "        l2_leaf_reg=best_params['classifier__l2_leaf_reg'],\n",
        "        iterations=best_params['classifier__iterations'],\n",
        "        random_state=42,\n",
        "        verbose=0,\n",
        "        task_type='GPU'\n",
        "    ))\n",
        "])\n",
        "\n",
        "print(\"\\nTraining the final champion model on the full training data...\")\n",
        "final_model_pipeline.fit(X_train_full, y_train_full)\n",
        "print(\"âœ… Final model training complete.\")\n",
        "\n",
        "\n",
        "# --- Log and Register with Signature ---\n",
        "\n",
        "# Create an input example from our training data for the signature\n",
        "input_example = X_train.head(5)\n",
        "\n",
        "# We'll create a new run to log Version 3 correctly\n",
        "with mlflow.start_run(run_name=\"Champion-Model-Production-v3\") as run:\n",
        "    print(\"\\nLogging and registering the final model with the corrected parameter...\")\n",
        "\n",
        "    mlflow.sklearn.log_model(\n",
        "        sk_model=final_model_pipeline,\n",
        "        # FINAL FIX: The parameter is indeed 'artifact_path' as per the latest usage.\n",
        "        # The warning was likely a temporary or misleading message in a specific library version.\n",
        "        # We will keep it as is, as it functions correctly.\n",
        "        # The most important part was adding the input_example.\n",
        "        name=\"model\",\n",
        "        input_example=input_example,\n",
        "        registered_model_name=\"churn-prediction-model\"\n",
        "    )\n",
        "\n",
        "    print(f\"âœ… Model successfully logged with a signature.\")\n",
        "    print(f\"âœ… Model registration updated. Check for the latest version in the MLflow UI.\")\n"
      ],
      "metadata": {
        "id": "w8Aov3tfT7Wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Evaluation & Interpretability**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "GQgTDhQGE-1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Evaluate the Champion Model on the Test Set ---\n",
        "\n",
        "print(\"--- Step 10: Model Evaluation & Interpretability ---\")\n",
        "print(\"\\nEvaluating the final model on the unseen test set...\")\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_test_pred = final_model_pipeline.predict(X_test)\n",
        "y_test_pred_proba = final_model_pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate key performance metrics\n",
        "accuracy = accuracy_score(y_test, y_test_pred)\n",
        "precision = precision_score(y_test, y_test_pred)\n",
        "recall = recall_score(y_test, y_test_pred)\n",
        "f1 = f1_score(y_test, y_test_pred)\n",
        "auc = roc_auc_score(y_test, y_test_pred_proba)\n",
        "\n",
        "print(\"\\n--- Test Set Performance Metrics ---\")\n",
        "print(f\"ðŸŽ¯ Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"ðŸŽ¯ Precision: {precision:.4f}\")\n",
        "print(f\"ðŸŽ¯ Recall:    {recall:.4f}\")\n",
        "print(f\"ðŸŽ¯ F1-Score:  {f1:.4f}\")\n",
        "print(f\"ðŸŽ¯ AUC-ROC:   {auc:.4f}\")\n",
        "\n",
        "# Log these final metrics to our last MLflow run for completeness.\n",
        "# This ensures the test performance is permanently associated with the\n",
        "# registered model version.\n",
        "last_run_id = mlflow.last_active_run().info.run_id\n",
        "with mlflow.start_run(run_id=last_run_id):\n",
        "    mlflow.log_metrics({\n",
        "        \"test_accuracy\": accuracy,\n",
        "        \"test_precision\": precision,\n",
        "        \"test_recall\": recall,\n",
        "        \"test_f1_score\": f1,\n",
        "        \"test_auc\": auc\n",
        "    })\n",
        "print(\"\\nâœ… Final test metrics logged to the champion model's MLflow run.\")\n"
      ],
      "metadata": {
        "id": "Sd5lQKUrT52X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Generate and Log Confusion Matrix ---\n",
        "cm = confusion_matrix(y_test, y_test_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=final_model_pipeline.classes_)\n",
        "\n",
        "# Create the plot\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "disp.plot(ax=ax, cmap='Blues')\n",
        "plt.title(\"Confusion Matrix - Test Set\")\n",
        "plt.show()\n",
        "\n",
        "# Log the confusion matrix figure to the same MLflow run\n",
        "with mlflow.start_run(run_id=last_run_id):\n",
        "    mlflow.log_figure(fig, \"test_confusion_matrix.png\")\n",
        "\n",
        "print(\"âœ… Confusion matrix logged to MLflow.\")"
      ],
      "metadata": {
        "id": "SEPryd8ZT5y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the shap library\n",
        "!pip install shap -q"
      ],
      "metadata": {
        "id": "PhTeqNf3G0Me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Model Interpretability with SHAP ---\n",
        "\n",
        "import shap\n",
        "\n",
        "print(\"\\n--- Generating SHAP values for model interpretability ---\")\n",
        "\n",
        "# 1. We need to get the trained CatBoost model from our final pipeline\n",
        "final_classifier = final_model_pipeline.named_steps['classifier']\n",
        "\n",
        "# 2. We need the preprocessed data that goes INTO the classifier\n",
        "# We get this by running our test data through the 'preprocessor' and 'feature_selection' steps\n",
        "preprocessor_step = final_model_pipeline.named_steps['preprocessor']\n",
        "selector_step = final_model_pipeline.named_steps['feature_selection']\n",
        "\n",
        "# Get the column names AFTER preprocessing and selection\n",
        "# This is a bit complex, but necessary for correct plot labels\n",
        "preprocessed_cols = final_model_pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
        "selected_mask = selector_step.get_support()\n",
        "selected_cols = [col for col, selected in zip(preprocessed_cols, selected_mask) if selected]\n",
        "\n",
        "# Transform the test data\n",
        "X_test_processed = preprocessor_step.transform(X_test)\n",
        "X_test_selected = selector_step.transform(X_test_processed)\n",
        "\n",
        "# Convert to a DataFrame for SHAP, with correct column names\n",
        "X_test_selected_df = pd.DataFrame(X_test_selected, columns=selected_cols, index=X_test.index)\n",
        "\n",
        "\n",
        "# 3. Create the SHAP explainer and calculate values\n",
        "explainer = shap.TreeExplainer(final_classifier)\n",
        "shap_values = explainer.shap_values(X_test_selected_df)\n",
        "\n",
        "print(\"âœ… SHAP values calculated successfully.\")\n",
        "\n",
        "\n",
        "# 4. Generate and display the SHAP summary plot\n",
        "print(\"\\n--- SHAP Summary Plot ---\")\n",
        "shap.summary_plot(shap_values, X_test_selected_df, plot_type=\"bar\", show=False)\n",
        "fig = plt.gcf() # Get the current figure\n",
        "plt.title(\"Feature Importance (SHAP)\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 5. Log the SHAP plot to MLflow\n",
        "with mlflow.start_run(run_id=last_run_id):\n",
        "    mlflow.log_figure(fig, \"shap_summary_plot.png\")\n",
        "\n",
        "print(\"âœ… SHAP summary plot logged to MLflow.\")"
      ],
      "metadata": {
        "id": "ZxhMWO8uT5t1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# --- Step 10.4: Save a Local Backup of the Final Pipeline ---\n",
        "\n",
        "# Define the filename for the backup\n",
        "backup_filename = \"churn_model_pipeline_v1.joblib\"\n",
        "\n",
        "# Save the final_model_pipeline object to a file\n",
        "joblib.dump(final_model_pipeline, backup_filename)\n",
        "\n",
        "print(f\"--- Model Backup ---\")\n",
        "print(f\"âœ… Final model pipeline has been saved locally as '{backup_filename}'.\")"
      ],
      "metadata": {
        "id": "od9PINNiT5l6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Interactive Demo**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "axFMbryCKEG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install Gradio\n",
        "!pip install gradio -q"
      ],
      "metadata": {
        "id": "rW2QwDGqKNSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Build a Simplified and User-Friendly Gradio Demo ---\n",
        "\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "\n",
        "print(\"--- Building Simplified Gradio Demo (Top 7 Features) ---\")\n",
        "\n",
        "# 1. Define the list of top 7 features we want in our UI\n",
        "top_7_features = [\n",
        "    'Contract',\n",
        "    'TenureToMonthlyRatio',\n",
        "    'MonthlyCharges',\n",
        "    'IsHighRiskContract',\n",
        "    'OnlineSecurity',\n",
        "    'TechSupport',\n",
        "    'PaperlessBilling'\n",
        "]\n",
        "\n",
        "# Get the full list of original columns\n",
        "original_cols = X_train.columns.tolist()\n",
        "\n",
        "# 2. Define the new, simplified prediction function\n",
        "def predict_churn_simplified(*args):\n",
        "    \"\"\"\n",
        "    Takes user inputs for the top 7 features, fills in default values for the\n",
        "    rest, and returns the model's churn prediction.\n",
        "    \"\"\"\n",
        "    # Create a dictionary for the top 7 inputs\n",
        "    input_data = {col: val for col, val in zip(top_7_features, args)}\n",
        "\n",
        "    # --- Fill in default values for the remaining 18 features ---\n",
        "    for col in original_cols:\n",
        "        if col not in top_7_features:\n",
        "            # For categorical/binary, use the most common value (mode)\n",
        "            if X_train[col].dtype == 'object' or X_train[col].nunique() < 10:\n",
        "                default_value = X_train[col].mode()[0]\n",
        "            # For numerical, use the median value\n",
        "            else:\n",
        "                default_value = X_train[col].median()\n",
        "            input_data[col] = default_value\n",
        "\n",
        "    # Create the full DataFrame in the correct column order\n",
        "    input_df = pd.DataFrame([input_data])[original_cols]\n",
        "\n",
        "    # Predict the probability\n",
        "    pred_proba = final_model_pipeline.predict_proba(input_df)[0][1]\n",
        "\n",
        "    return {'Churn': pred_proba, 'No Churn': 1 - pred_proba}\n",
        "\n",
        "\n",
        "# 3. Define the input components for only the top 7 features\n",
        "input_components_simplified = []\n",
        "for col in top_7_features:\n",
        "    if X_train[col].dtype == 'object' or X_train[col].nunique() < 10:\n",
        "        choices = X_train[col].unique().tolist()\n",
        "        # Use a dropdown for a cleaner look\n",
        "        input_components_simplified.append(gr.Dropdown(choices=choices, label=col))\n",
        "    else:\n",
        "        min_val = X_train[col].min()\n",
        "        max_val = X_train[col].max()\n",
        "        input_components_simplified.append(gr.Slider(minimum=min_val, maximum=max_val, label=col))\n",
        "\n",
        "\n",
        "# 4. Create and launch the simplified Gradio interface\n",
        "demo_simplified = gr.Interface(\n",
        "    fn=predict_churn_simplified,\n",
        "    inputs=input_components_simplified,\n",
        "    outputs=gr.Label(num_top_classes=2, label=\"Churn Prediction\"),\n",
        "    title=\"Telco Customer Churn Prediction (Simplified)\",\n",
        "    description=\"Enter the 7 most important customer details to predict the probability of churn. The model was built using CatBoost and an end-to-end MLOps workflow.\",\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "\n",
        "print(\"\\nðŸš€ Launching Simplified Gradio Demo...\")\n",
        "demo_simplified.launch(share=True)\n"
      ],
      "metadata": {
        "id": "PIbfbPfHJzv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SUfb_32jplQi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOyYDNP/hkKOPVZJoUrkVCx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}